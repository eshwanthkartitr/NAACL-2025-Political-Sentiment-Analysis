# Political Sentiment Analysis - NAACL DravidaLangTech (Task 4)

This repository contains the code and models used for Task 4 (Political Sentiment Analysis) in the **NAACL DravidaLangTech 2025** competition. The objective of this task is to classify political sentiment in Tamil code-mixed tweets.

## ğŸ“Œ Project Overview  
The dataset consists of code-mixed Dravidian text labeled for political sentiment (7 classes). Various machine learning and deep learning models have been explored to achieve the best classification performance.

## ğŸš€ Best Performing Model  
The best-performing model in this project is **LaBSE + SVM**, which achieved the highest F1 Score among all tested approaches.

## ğŸ“‚ Repository Structure  

```
ğŸ“¦ Project Root  
 â”ƒ â”£ ğŸ“œ .gitignore  
 â”ƒ â”£ ğŸ“œ bert_base_cased.ipynb     # Fine-tuned BERT-base-cased for classification  
 â”ƒ â”£ ğŸ“œ cleaned_PS_train.csv      # Preprocessed training dataset  
 â”ƒ â”£ ğŸ“œ cleaned_PS_dev.csv        # Preprocessed validation dataset  
 â”ƒ â”£ ğŸ“œ cleaned_PS_test.csv       # Preprocessed test dataset  
 â”ƒ â”£ ğŸ“œ fasttext.ipynb            # FastText-based classification  
 â”ƒ â”£ ğŸ“œ indic_bert.ipynb          # IndicBERT fine-tuning  
 â”ƒ â”£ ğŸ“œ indic_bert_nohashtag.ipynb # IndicBERT without hashtags  
 â”ƒ â”£ ğŸ“œ muril_nohashtag.ipynb     # MuRIL without hashtags  
 â”ƒ â”£ ğŸ“œ preprocess.ipynb          # Data preprocessing pipeline  
 â”ƒ â”£ ğŸ“œ PS_train.csv              # Original training dataset  
 â”ƒ â”£ ğŸ“œ PS_dev.csv                # Original validation dataset  
 â”ƒ â”£ ğŸ“œ PS_test_without_labels.csv # Test dataset without labels  
 â”ƒ â”£ ğŸ“œ submission.csv            # Final submission file  
 â”ƒ â”£ ğŸ“œ svm_sbert.ipynb           # SVM model using SBERT embeddings  
 â”ƒ â”£ ğŸ“œ tamil_sbert_nohashtag.ipynb # Tamil SBERT without hashtags  
```

## ğŸ† Models Used  

- **LaBSE + SVM (Best Model)**
- **BERT-base-cased**  
- **IndicBERT**  
- **MuRIL**  
- **FastText**  
- **SBERT-based models**  
- **SVM-based models**  
- **XLM-Roberta**

All SVM-based models, are implemented in `svm.ipynb` where embeddings are first extracted and then a SVM classifier is trained. The other notebooks correspond to fine-tuned versions of their respective models for classification.

## ğŸ› ï¸ Setup & Usage  

### 1ï¸âƒ£ Install Dependencies  
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run Preprocessing  
```python
# Open preprocess.ipynb and execute the preprocessing pipeline
```

### 3ï¸âƒ£ Train Models  
Execute the respective notebooks (`.ipynb`) for training different models.


## ğŸ“ Notes  
- LaBSE+SVM performed the best among all models.  
- Other SVM models are located in `svm.ipynb`.  
- Different models were fine-tuned for classification based on their respective architectures.  
- Due to dataset scarcity SVM based methods performed better

## ğŸ“§ Contact  
For any queries, feel free to reach out.

- [@nithishariyha](https://github.com/ariyha)  
- [@eshwanthkarti](github.com/eshwanthkartitr)  
- [@vikash](https://github.com/vikash22092004)
- [@yeshwanthbalaji](https://github.com/YeshwanthBalaji2022)


---

